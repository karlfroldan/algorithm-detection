{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsrunFvIkoPN"
      },
      "outputs": [],
      "source": [
        "!pip install folium==0.2.1 torch neptune-client==0.15.2 tqdm > /dev/null \n",
        "!pip install transformers==2.1.1 pytorch-lightning==1.5.10  > /dev/null\n",
        "!apt install git git-lfs > /dev/null\n",
        "!git clone https://github.com/karlfroldan/prototype.git\n",
        "!git clone https://huggingface.co/microsoft/codebert-base codebert\n",
        "!mv prototype/* . \n",
        "!rm -rf prototype \n",
        "%cd codebert \n",
        "!git lfs install\n",
        "!git lfs pull \n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99ec1ZRImN_4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from pytorch_lightning.loggers.neptune import NeptuneLogger\n",
        "\n",
        "import neptune.new as neptune\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor\n",
        "\n",
        "import transformers\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "\n",
        "from torchvision.ops import sigmoid_focal_loss\n",
        "\n",
        "from prototype_dataloader import get_datasets\n",
        "\n",
        "from sklearn.metrics import f1_score, hamming_loss\n",
        "import warnings\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'using device: {device}')\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    \"\"\"\"\n",
        "    Seed everything.\n",
        "    \"\"\"   \n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    pl.seed_everything(seed)\n",
        "\n",
        "# Set the RNG\n",
        "seed_everything(1729)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO8pLokAPNXd"
      },
      "source": [
        "### Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciDV0xPEmrVm"
      },
      "outputs": [],
      "source": [
        "labels = [\"quicksort\", \"mergesort\", \"selectionsort\", \"insertionsort\", \"bubblesort\", \n",
        "            \"linearsearch\", \"binarysearch\", \"linkedlist\", \"hashmap\"] #['selectionsort', 'bubblesort', 'binarysearch']\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"./codebert\")\n",
        "model = RobertaModel.from_pretrained(\"./codebert\")\n",
        "\n",
        "data_csv = pd.read_csv(\"prototype.csv\")\n",
        "train_set, test_set = get_datasets(data_csv, tokenizer, split=0.1, data_folder='./data/prototype', labels=labels)\n",
        "len(train_set), len(test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUqDe900oToJ"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOz4N9WFng4M"
      },
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.01)\n",
        "\n",
        "class OurModel(pl.LightningModule):\n",
        "    def __init__(self, codebert, loss, input=393_216, hidden=None, labels=9, train_rate=1e-3, device='cuda'):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.transformer = codebert.to(device)\n",
        "\n",
        "        # RoBERTa has 12 encoding layers. For this study, let's freeze the first 9\n",
        "        # and retrain the last 3\n",
        "        layers = [self.transformer.embeddings, *self.transformer.encoder.layer[:9]]\n",
        "        for layer in layers: #self.transformer.parameters():\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        self.loss = loss\n",
        "        self.train_rate = train_rate\n",
        "\n",
        "        layers = [nn.Dropout(p=0.1), nn.Linear(768 * 512, 420), nn.BatchNorm1d(420), nn.ReLU()]\n",
        "\n",
        "        # self.dropout = nn.Dropout(p=0.1).to(device)\n",
        "\n",
        "        # self.fc1 = nn.Linear(768 * 512, 420).to(device)\n",
        "        \n",
        "        # self.bn1 = nn.BatchNorm1d(420).to(device)\n",
        "\n",
        "        # init_weights(self.fc1)\n",
        "\n",
        "        \n",
        "\n",
        "        self.hidden_is_none = hidden is None\n",
        "        last = 420\n",
        "        if hidden is not None:\n",
        "            #self.hidden = []\n",
        "            #self.batch_norms = []\n",
        "\n",
        "            for i in hidden:\n",
        "                layers.append(nn.Dropout(p=0.1))\n",
        "                layers.append(nn.Linear(last, i)) \n",
        "                layers.append(nn.BatchNorm1d(420))\n",
        "                layers.append(nn.ReLU())\n",
        "                # n = nn.Linear(last, i).to(device)\n",
        "                # bn = nn.BatchNorm1d(i).to(device)\n",
        "\n",
        "                # init_weights(n)\n",
        "                # self.hidden.append(n)\n",
        "                # self.batch_norms.append(bn)\n",
        "                last = i\n",
        "        layers.append(nn.Linear(last, labels)) \n",
        "        \n",
        "        # self.output = nn.Linear(last, labels).to(device)\n",
        "        # init_weights(self.output)\n",
        "\n",
        "        for layer in layers:\n",
        "            init_weights(layer)\n",
        "\n",
        "        self.ann = nn.Sequential(*layers).to(device)\n",
        "\n",
        "    def get_preds(self, y):\n",
        "        return (y >= 0.5).long()\n",
        "      \n",
        "    def get_preds_numpy(self, y):\n",
        "        return (y >= 0.5).astype(int)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass the inputs to the transformer\n",
        "        (out, mask) = self.transformer(x)\n",
        "\n",
        "        # Flatten the transformer's output so we can plug it into the\n",
        "        # simple feedforward neural network.\n",
        "        out = torch.flatten(out, 1)\n",
        "\n",
        "        #out = self.bn1(F.relu(self.fc1(out)))\n",
        "        #out = F.relu(self.bn1(self.fc1(self.dropout(out))))\n",
        "        return self.ann(out)\n",
        "        # if not self.hidden_is_none:\n",
        "        #     for layer, bn in zip(self.hidden, self.batch_norms):\n",
        "        #         #out = bn(F.relu(layer(out)))\n",
        "        #         out = F.relu(bn(layer(self.dropout(out))))\n",
        "\n",
        "        # Instead, we need to ensure that we add a sigmoid layer\n",
        "        # when training the model.\n",
        "        #return self.output(out)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), weight_decay=1e-6, lr=self.train_rate)\n",
        "        #scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
        "        #self.scheduler = scheduler\n",
        "        return optimizer\n",
        "        #return [optimizer], [self.scheduler]\n",
        "        \n",
        "    def training_step(self, train_batch, batch_idx):\n",
        "        X, y = train_batch\n",
        "        X = X['input_ids']\n",
        "        y_hat = self(X)\n",
        "\n",
        "        #print(y_hat)\n",
        "        \n",
        "        loss = self.loss(y_hat, y)\n",
        "        self.log('train loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, val_batch, batch_idx):\n",
        "        X, y = val_batch\n",
        "        X = X['input_ids']\n",
        "        y_hat = self(X)\n",
        "        loss = self.loss(y_hat, y)\n",
        "        y_hat = torch.sigmoid(y_hat)\n",
        "\n",
        "        self.log('validation loss', loss)\n",
        "        # Transfer them to the CPU\n",
        "\n",
        "        y_cpu = y.squeeze().cpu().detach().numpy()\n",
        "        y_hat_sigmoid_cpu = self.get_preds(y_hat).squeeze().cpu().detach().numpy()\n",
        "\n",
        "        hamming = hamming_loss(y_cpu, y_hat_sigmoid_cpu)\n",
        "\n",
        "        f1_micro = f1_score(y_cpu, y_hat_sigmoid_cpu, average='micro', zero_division=1)\n",
        "        f1_macro = f1_score(y_cpu, y_hat_sigmoid_cpu, average='macro', zero_division=1)\n",
        "        self.log('hamming loss', hamming)\n",
        "        self.log('Micro F1', f1_micro)\n",
        "        self.log('Macro F1', f1_macro)\n",
        "        #self.log('Learning rate', self.scheduler.get_last_lr()[len(self.scheduler.get_last_lr()) - 1])\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCMiPURIogRe"
      },
      "source": [
        "### The Loss Function\n",
        "\n",
        "The criterion that this model will use is the **Focal Loss** which is defined as an extension of the **Cross-entropy loss**. \n",
        "\n",
        "We know that Cross-entropy loss is defined as "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSTYk-SHoea_"
      },
      "outputs": [],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=4, alpha=0.1, device='cuda', labels=9):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = torch.tensor([alpha, 1 - alpha])\n",
        "        if device == 'cuda':\n",
        "          self.alpha = self.alpha.to(device)\n",
        "        self.gamma = gamma\n",
        "        self.labels = labels\n",
        "\n",
        "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
        "    \n",
        "    def forward(self, y_hat, y):\n",
        "        epsilon = 1e-4\n",
        "        y_prime = y.type(torch.float32)\n",
        "        y_hat = y_hat.type(torch.float32)\n",
        "\n",
        "        b = self.bce(y_hat, y_prime)\n",
        "        # alpha_t = self.alpha.gather(0, y.data.view(-1)).reshape(-1, 9)\n",
        "\n",
        "        alpha_t = self.alpha.gather(0, y.data.view(-1)).reshape(-1, self.labels)\n",
        "        p_t = torch.exp(-b + epsilon)\n",
        "        \n",
        "        F_loss = alpha_t * (1 - p_t) ** self.gamma * b\n",
        "\n",
        "        return F_loss.mean().type(torch.float16)\n",
        "\n",
        "class WeightedCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, weight=None, pos_weight=None):\n",
        "        super(WeightedCrossEntropyLoss, self).__init__()\n",
        "        self.bce = nn.BCEWithLogitsLoss(weight=weight, pos_weight=pos_weight)\n",
        "    def forward(self, y_hat, y):\n",
        "        y = y.type(torch.float32)\n",
        "        y_hat = y_hat.type(torch.float32)\n",
        "        return self.bce(y_hat, y).type(torch.float16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYLVpB-polxz"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULWx-yg5ojrt"
      },
      "outputs": [],
      "source": [
        "weights = torch.tensor([0.9875727720555306, 0.9703313927451859, 0.7831392745185849, 1.0, 0.9658531124048365,\n",
        "    0.7736229287953426, 0.5107478728168383, 0.38434841021047916, 0.36990595611285265])\n",
        "\n",
        "wce_loss = WeightedCrossEntropyLoss(weight=weights)\n",
        "#focal_loss = FocalLoss(gamma=5, alpha=0.25, device='cuda', labels=len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_set, batch_size=32, num_workers=2, shuffle=True)\n",
        "test_dataloader = DataLoader(test_set, batch_size=64, num_workers=2, shuffle=False)"
      ],
      "metadata": {
        "id": "dYBJ_hoje3yQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neptune_logger = NeptuneLogger(\n",
        "    api_key=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI1NDBhOTI3OC0yOTZmLTQ4YmYtOTVjNC00MTIzZjJjMGM3Y2MifQ==\",  # replace with your own\n",
        "    project=\"pancit-canton/augmented-runs\" # \"<WORKSPACE/PROJECT>\"\n",
        ")\n",
        "\n",
        "m = OurModel(model, loss=wce_loss, train_rate=0.0001, hidden=None, device='cuda', labels=len(labels))\n",
        "trainer = pl.Trainer(gpus=1, precision=16, max_epochs=6, log_every_n_steps=3, logger=neptune_logger, enable_checkpointing=False)\n",
        "trainer.fit(m, train_dataloader, test_dataloader)"
      ],
      "metadata": {
        "id": "7S9P_VS7fXPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wcL1nFSqvJ9"
      },
      "source": [
        "The hamming loss is defined as \n",
        "$$\n",
        "\\frac{1}{|N|\\cdot|L|}\\sum_{i=1}^{|N|}\\sum_{j=1}^{|L|}\\left(\\widehat{y}_{i,j}\\oplus y_{i,j}\\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCSDcryUF7dV"
      },
      "outputs": [],
      "source": [
        "prediction = []\n",
        "real = []\n",
        "subset_acc = 0\n",
        "subset_accuracy = lambda y_hat, y: torch.all((y == y_hat)).float()\n",
        "subset_pred = lambda y_hat, y: (torch.all(get_preds(y_hat) == y)).float()\n",
        "get_preds = lambda ys : (ys >= 0.5).long()\n",
        "\n",
        "#device = 'cpu'\n",
        "m.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    m = m.to(device)\n",
        "    for X, y in tqdm(test_dataloader):\n",
        "        X = X['input_ids'].to(device)\n",
        "        y_hat = torch.sigmoid(m(X))\n",
        "        prediction.append(y_hat.cpu().detach().numpy())\n",
        "        real.append(y.cpu().detach().numpy())\n",
        "\n",
        "\n",
        "prediction = np.vstack(prediction)\n",
        "real = np.vstack(real)\n",
        "\n",
        "import pickle as pkl \n",
        "\n",
        "for fname, array in zip([f'prediction_wce.pkl', f'real_wce.pkl'], [prediction, real]):\n",
        "    with open(fname, 'wb') as f:\n",
        "        pkl.dump(array, f)\n",
        "        print(f'Dumped {fname}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLhWY32YRIHR"
      },
      "outputs": [],
      "source": [
        "prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxw8ymoKRItx"
      },
      "outputs": [],
      "source": [
        "real"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlCm9utUb_-4"
      },
      "outputs": [],
      "source": [
        "np.mean(np.logical_and(real == 1, prediction >= 0.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjmjtStmela7"
      },
      "outputs": [],
      "source": [
        "np.sum(real == 1), np.sum(real == 1) + np.sum(real == 0), np.sum(real == 1) / (np.sum(real == 1) + np.sum(real == 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYwwWJukena9"
      },
      "outputs": [],
      "source": [
        "true_positives = np.sum(np.logical_and(real == 1, prediction >= 0.5))\n",
        "all_positives = np.sum(real == 1)\n",
        "false_positives = np.sum(np.logical_and(real == 0, prediction >= 0.5))\n",
        "print(f\"All Positives: {all_positives} | True Positives: {true_positives} | False Positives: {false_positives}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9ABhw1derom"
      },
      "outputs": [],
      "source": [
        "subset_all = np.all(np.equal(real == 1, prediction >= 0.5), axis=1)\n",
        "subset_acc = np.sum(subset_all) / len(subset_all)\n",
        "subset_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXfRKAnFeu7r"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Final Model WCE - Augmented2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}