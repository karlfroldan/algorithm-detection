{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsrunFvIkoPN"
      },
      "outputs": [],
      "source": [
        "!pip install folium==0.2.1 torch neptune-client==0.15.2 tqdm > /dev/null \n",
        "!pip install transformers==2.1.1 pytorch-lightning==1.5.10  > /dev/null\n",
        "!apt install git git-lfs > /dev/null\n",
        "!git clone https://github.com/karlfroldan/prototype.git\n",
        "!git clone https://huggingface.co/microsoft/codebert-base codebert\n",
        "!mv prototype/* . \n",
        "!rm -rf prototype \n",
        "%cd codebert \n",
        "!git lfs install\n",
        "!git lfs pull \n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99ec1ZRImN_4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "from prototype_dataloader import get_datasets\n",
        "from sklearn.metrics import f1_score, hamming_loss\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'using device: {device}')\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    \"\"\"\"\n",
        "    Seed everything.\n",
        "    \"\"\"   \n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    pl.seed_everything(seed)\n",
        "\n",
        "# Set the RNG\n",
        "seed_everything(1729)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO8pLokAPNXd"
      },
      "source": [
        "### Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciDV0xPEmrVm"
      },
      "outputs": [],
      "source": [
        "labels = [\"quicksort\", \"mergesort\", \"selectionsort\", \"insertionsort\", \"bubblesort\", \n",
        "            \"linearsearch\", \"binarysearch\", \"linkedlist\", \"hashmap\"] #['selectionsort', 'bubblesort', 'binarysearch']\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"./codebert\")\n",
        "model = RobertaModel.from_pretrained(\"./codebert\")\n",
        "\n",
        "data_csv = pd.read_csv(\"prototype.csv\")\n",
        "train_set, test_set = get_datasets(data_csv, tokenizer, split=0.1, data_folder='./data/prototype', labels=labels)\n",
        "len(train_set), len(test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUqDe900oToJ"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOz4N9WFng4M"
      },
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.01)\n",
        "\n",
        "class OurModel(pl.LightningModule):\n",
        "    def __init__(self, codebert, loss, input=393_216, hidden=None, labels=9, train_rate=1e-3, device='cuda'):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.transformer = codebert.to(device)\n",
        "\n",
        "        # RoBERTa has 12 encoding layers. For this study, let's freeze the first 9\n",
        "        # and retrain the last 3\n",
        "        layers = [self.transformer.embeddings, *self.transformer.encoder.layer[:9]]\n",
        "        for layer in layers:\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        self.loss = loss\n",
        "        self.train_rate = train_rate\n",
        "\n",
        "        layers = [nn.Dropout(p=0.1), nn.Linear(768 * 512, 420), nn.BatchNorm1d(420), nn.ReLU()]\n",
        "        \n",
        "        self.hidden_is_none = hidden is None\n",
        "        last = 420\n",
        "        if hidden is not None:\n",
        "\n",
        "            for i in hidden:\n",
        "                layers.append(nn.Dropout(p=0.1))\n",
        "                layers.append(nn.Linear(last, i)) \n",
        "                layers.append(nn.BatchNorm1d(i))\n",
        "                layers.append(nn.ReLU())\n",
        "                last = i\n",
        "        layers.append(nn.Linear(last, labels)) \n",
        "        for layer in layers:\n",
        "            init_weights(layer)\n",
        "\n",
        "        self.ann = nn.Sequential(*layers).to(device)\n",
        "\n",
        "    def get_preds(self, y):\n",
        "        return (y >= 0.5).long()\n",
        "      \n",
        "    def get_preds_numpy(self, y):\n",
        "        return (y >= 0.5).astype(int)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass the inputs to the transformer\n",
        "        (out, mask) = self.transformer(x)\n",
        "\n",
        "        # Flatten the transformer's output so we can plug it into the\n",
        "        # simple feedforward neural network.\n",
        "        out = torch.flatten(out, 1)\n",
        "        return self.ann(out)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), weight_decay=1e-6, lr=self.train_rate)\n",
        "        return optimizer\n",
        "        \n",
        "    def training_step(self, train_batch, batch_idx):\n",
        "        X, y = train_batch\n",
        "        X = X['input_ids']\n",
        "        y_hat = self(X)\n",
        "        loss = self.loss(y_hat, y)\n",
        "        self.log('train loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, val_batch, batch_idx):\n",
        "        X, y = val_batch\n",
        "        X = X['input_ids']\n",
        "        y_hat = self(X)\n",
        "        loss = self.loss(y_hat, y)\n",
        "        y_hat = torch.sigmoid(y_hat)\n",
        "\n",
        "        self.log('validation loss', loss)\n",
        "        # Transfer them to the CPU\n",
        "\n",
        "        y_cpu = y.squeeze().cpu().detach().numpy()\n",
        "        y_hat_sigmoid_cpu = self.get_preds(y_hat).squeeze().cpu().detach().numpy()\n",
        "\n",
        "        hamming = hamming_loss(y_cpu, y_hat_sigmoid_cpu)\n",
        "\n",
        "        f1_micro = f1_score(y_cpu, y_hat_sigmoid_cpu, average='micro', zero_division=1)\n",
        "        f1_macro = f1_score(y_cpu, y_hat_sigmoid_cpu, average='macro', zero_division=1)\n",
        "        self.log('hamming loss', hamming)\n",
        "        self.log('Micro F1', f1_micro)\n",
        "        self.log('Macro F1', f1_macro)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCMiPURIogRe"
      },
      "source": [
        "### The Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSTYk-SHoea_"
      },
      "outputs": [],
      "source": [
        "class WeightedCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, weight=None, pos_weight=None):\n",
        "        super(WeightedCrossEntropyLoss, self).__init__()\n",
        "        self.bce = nn.BCEWithLogitsLoss(weight=weight, pos_weight=pos_weight)\n",
        "    def forward(self, y_hat, y):\n",
        "        y = y.type(torch.float32)\n",
        "        y_hat = y_hat.type(torch.float32)\n",
        "        return self.bce(y_hat, y).type(torch.float16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYLVpB-polxz"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULWx-yg5ojrt"
      },
      "outputs": [],
      "source": [
        "weights = torch.tensor([0.9875727720555306, 0.9703313927451859, 0.7831392745185849, 1.0, 0.9658531124048365,\n",
        "    0.7736229287953426, 0.5107478728168383, 0.38434841021047916, 0.36990595611285265])\n",
        "\n",
        "wce_loss = WeightedCrossEntropyLoss(weight=weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYBJ_hoje3yQ"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_set, batch_size=32, num_workers=2, shuffle=True)\n",
        "test_dataloader = DataLoader(test_set, batch_size=64, num_workers=2, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S9P_VS7fXPw"
      },
      "outputs": [],
      "source": [
        "m = OurModel(model, loss=wce_loss, train_rate=0.0001, hidden=None, device='cuda', labels=len(labels))\n",
        "trainer = pl.Trainer(gpus=1, precision=16, max_epochs=6, log_every_n_steps=3, enable_checkpointing=False)\n",
        "trainer.fit(m, train_dataloader, test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXfRKAnFeu7r"
      },
      "outputs": [],
      "source": [
        "torch.save(m.state_dict(), \"combined_model.pth\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Final Model WCE - Augmented2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
